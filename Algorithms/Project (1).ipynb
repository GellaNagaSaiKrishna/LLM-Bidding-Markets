{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fc4c58-2638-4053-8b41-ad21de96fa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hare Krishna\n"
     ]
    }
   ],
   "source": [
    "print(\"Hare Krishna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8745c2-209b-4796-8e3b-1fbe702b0301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. INSTALL AND SETUP (Run these lines first if you haven't already) ---\n",
    "# !pip install accelerate transformers pandas torch numpy scipy\n",
    "from huggingface_hub import login\n",
    "login(token='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')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d62e89a8-2533-49bf-8515-93ff73dec5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress transformers warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# --- GLOBAL CONFIGURATION ---\n",
    "LLM_CONFIGS = {\n",
    "    # Model ID, Name, and Mock Token Cost (for demonstration)\n",
    "    \"LLM_1B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        \"name\": \"1B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000005, # Mock cost: cheaper model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "        \"ucb_N\": 0,    # N: Number of times this arm has been selected\n",
    "        \"ucb_Q\": 0.0,  # Q: Total utility (Reward - Cost) received\n",
    "        \"ucb_mean_reward\": 0.0, # Q/N: Average utility (Note: Key kept as 'reward' for simplicity)\n",
    "    },\n",
    "    \"LLM_3B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        \"name\": \"3B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000025, # Mock cost: more expensive model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "        \"ucb_N\": 0,\n",
    "        \"ucb_Q\": 0.0,\n",
    "        \"ucb_mean_reward\": 0.0,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6431844-5445-49c0-9bca-54e7531586e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCB exploration parameter\n",
    "UCB_C = 0.5\n",
    "\n",
    "# Mapping columns to rewards for a correct prediction\n",
    "REWARD_MAP = {\n",
    "    'Capital City': 1,\n",
    "    'Continent': 1,\n",
    "    'Latitude': 2,\n",
    "    'Longitude': 2\n",
    "}\n",
    "EVAL_COLUMNS = list(REWARD_MAP.keys())\n",
    "\n",
    "# Dataset configuration\n",
    "# IMPORTANT: Use the exact path to your CSV file\n",
    "file_path = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv'\n",
    "#file_path = 'all capital cities in the world.csv' # Placeholder for a common file structure\n",
    "QUERY_COLUMN = 'Country'\n",
    "\n",
    "# Global counter for the total number of rounds (t in UCB)\n",
    "GLOBAL_T = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be4bd42-0676-41ec-984f-fa8c8258798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. LLM INITIALIZATION ---\n",
    "def initialize_llms():\n",
    "    \"\"\"Initializes both Llama 3 models.\"\"\"\n",
    "    global LLM_CONFIGS\n",
    "    print(\"Initializing LLM pipelines...\")\n",
    "    \n",
    "    for key, config in LLM_CONFIGS.items():\n",
    "        try:\n",
    "            print(f\"Loading {config['name']} ({config['model_id']})...\")\n",
    "            # Using low-precision dtype and device_map requires 'accelerate'\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=config['model_id'],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            config[\"pipe\"] = pipe\n",
    "            config[\"tokenizer\"] = pipe.tokenizer\n",
    "            print(f\"{config['name']} loaded successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFATAL: Failed to load {config['name']} pipeline. Check environment, token, and hardware.\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a287a6c1-3f40-4511-bac3-832317195707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. LLM PREDICTION AND COST CALCULATION ---\n",
    "def get_llm_prediction_and_cost(country_name, llm_key):\n",
    "    \"\"\"\n",
    "    Queries the specified Llama 3 pipeline, returns data, raw output, and mock cost.\n",
    "    \"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    pipe = config[\"pipe\"]\n",
    "    pipe_tokenizer = config[\"tokenizer\"]\n",
    "    cost_per_token = config[\"cost_per_token\"]\n",
    "\n",
    "    # 2.1 Construct Prompt\n",
    "    prompt_instruction = f\"\"\"\n",
    "    You are an expert geographical information system. \n",
    "    Your task is to provide the Capital City, Continent, Latitude, and Longitude for the requested country.\n",
    "    You MUST respond ONLY with a valid JSON object. DO NOT include any text outside the JSON object.\n",
    "    The JSON structure must be: {{\"Capital City\": \"...\", \"Continent\": \"...\", \"Latitude\": \"...\", \"Longitude\": \"...\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Provide the geographical data for: {country_name}\"},\n",
    "    ]\n",
    "\n",
    "    # Apply chat template for Llama 3 format\n",
    "    prompt = pipe_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "     # 2.2 Run Inference\n",
    "    terminators = [\n",
    "        pipe_tokenizer.eos_token_id,\n",
    "        pipe_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    # Run Inference with deterministic settings\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # 2.3 Extract and Parse the JSON\n",
    "    raw_output = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "    \n",
    "    json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
    "    \n",
    "    llm_response_dict = {col: \"\" for col in EVAL_COLUMNS}\n",
    "    \n",
    "    if json_match:\n",
    "        json_string = json_match.group(0)\n",
    "        try:\n",
    "            llm_response_dict = json.loads(json_string)\n",
    "        except json.JSONDecodeError:\n",
    "            pass # Keep default empty dict if parsing fails\n",
    "            \n",
    "    # 2.4 Mock Cost Calculation\n",
    "    # Token count estimation: 1 token is roughly 4 characters\n",
    "    prompt_tokens = len(prompt) // 4\n",
    "    response_tokens = len(raw_output) // 4\n",
    "    total_tokens = prompt_tokens + response_tokens\n",
    "    \n",
    "    cost = total_tokens * cost_per_token\n",
    "\n",
    "    return llm_response_dict, raw_output, total_tokens, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83159154-562e-46a9-994e-3c983427bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. UCB AND MYERSON LOGIC (CORRECTED) ---\n",
    "\n",
    "def calculate_virtual_valuation(llm_key, country_name, current_t, total_reward, total_cost):\n",
    "    \"\"\"\n",
    "    Calculates the Virtual Valuation for a given LLM's result.\n",
    "    The winner will be the one with the *HIGHEST* Virtual Valuation to maximize utility.\n",
    "\n",
    "    a = -Reward + Cost - C * sqrt(ln(t)/N) (Adjusted Utility/Bid)\n",
    "    \"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Step 1: Calculate 'a' (Adjusted Utility/Bid)\n",
    "    N_eff = max(config[\"ucb_N\"], 1)\n",
    "    \n",
    "    # 'a' is the current Utility (Reward - Cost) + Exploration Bonus\n",
    "    a = -total_reward + total_cost - UCB_C * math.sqrt(math.log(current_t) / N_eff)\n",
    "    \n",
    "    # Step 2: Calculate CDF(a) and PDF(a)\n",
    "    try:\n",
    "        pdf_a = norm.pdf(a)\n",
    "        cdf_a = norm.cdf(a)\n",
    "    except ValueError:\n",
    "        # Catch edge cases where 'a' is extreme\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "    # Step 3: Calculate Virtual Valuation (Myerson for Utility Maximization)\n",
    "    # V(a) = a + CDF(a) / PDF(a) - Selecting the HIGHEST V(a) maximizes utility.\n",
    "    \n",
    "    if pdf_a == 0:\n",
    "        # Assign a high valuation if PDF is zero (extreme 'a' value)\n",
    "        virtual_valuation = float('inf')\n",
    "    else:\n",
    "        # We use V(a) = a + (cdf_a / pdf_a) and select max(V) to maximize utility\n",
    "        virtual_valuation = a + (cdf_a / pdf_a)\n",
    "    return virtual_valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1aeb27-4684-42db-888e-a30075de9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ucb_stats(llm_key, utility):\n",
    "    \"\"\"Updates the UCB statistics for the winning LLM arm based on Net Utility (Reward - Cost).\"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Q now tracks total utility (Reward - Cost)\n",
    "    config[\"ucb_Q\"] += utility\n",
    "    config[\"ucb_N\"] += 1\n",
    "    \n",
    "    # mean_reward now tracks mean utility\n",
    "    config[\"ucb_mean_reward\"] = config[\"ucb_Q\"] / config[\"ucb_N\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f66dd4-bc1d-4c87-9b70-037697fa6e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. EVALUATION LOGIC (Main loop - CORRECTED) ---\n",
    "def calculate_efficiency_with_ucb_myerson(df, query_col):\n",
    "    \"\"\"\n",
    "    Loops through the dataset, gets predictions from both LLMs, \n",
    "    applies UCB/Myerson logic to select the winner, and calculates overall efficiency.\n",
    "    \"\"\"\n",
    "    global GLOBAL_T\n",
    "    \n",
    "    # Data cleaning for ground truth\n",
    "    for col in EVAL_COLUMNS:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "    total_count = len(df)\n",
    "    \n",
    "    # Tracking for final results\n",
    "    correct_counts = {col: 0 for col in EVAL_COLUMNS}\n",
    "    llm_selection_counts = {key: 0 for key in LLM_CONFIGS.keys()}\n",
    "    total_reward_collected = 0\n",
    "    total_cost_incurred = 0\n",
    "\n",
    "    print(f\"Starting UCB/Reverse-Myerson evaluation on {total_count} countries...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        country = row[query_col]\n",
    "        GLOBAL_T += 1 # Increment total rounds (t)\n",
    "\n",
    "        # Dictionary to hold the results for both LLMs in this round\n",
    "        round_results = {}\n",
    "\n",
    "         # 4.1 Get Predictions, Rewards, and Costs for BOTH LLMs\n",
    "        for llm_key in LLM_CONFIGS.keys():\n",
    "            # Run inference for the current LLM\n",
    "            llm_response_dict, raw_output, total_tokens, cost = \\\n",
    "                get_llm_prediction_and_cost(country, llm_key)\n",
    "            \n",
    "            # Calculate Total Reward for this LLM's prediction\n",
    "            current_reward = 0\n",
    "            is_correct_for_llm = {col: False for col in EVAL_COLUMNS}\n",
    "            \n",
    "            for col in EVAL_COLUMNS:\n",
    "                true_value = row[col]\n",
    "                predicted_value = str(llm_response_dict.get(col, '')).strip().lower()\n",
    "                \n",
    "                is_correct = (predicted_value == true_value)\n",
    "                \n",
    "                # Robust Comparison for Latitude/Longitude (Tolerance 0.05)\n",
    "                if col in ['Latitude', 'Longitude']:\n",
    "                    true_num_str = re.sub(r'[^0-9.-]', '', true_value)\n",
    "                    pred_num_str = re.sub(r'[^0-9.-]', '', predicted_value)\n",
    "                    \n",
    "                    try:\n",
    "                        true_num = float(true_num_str)\n",
    "                        pred_num = float(pred_num_str)\n",
    "                        \n",
    "                        if abs(true_num - pred_num) < 0.05:\n",
    "                            is_correct = True\n",
    "                        else:\n",
    "                            is_correct = False\n",
    "                    except ValueError:\n",
    "                        is_correct = False\n",
    "                if is_correct:\n",
    "                    current_reward += REWARD_MAP[col]\n",
    "                    is_correct_for_llm[col] = True\n",
    "            round_results[llm_key] = {\n",
    "                \"reward\": current_reward,\n",
    "                \"cost\": cost,\n",
    "                \"utility\": current_reward - cost, # Store Net Utility\n",
    "                \"is_correct\": is_correct_for_llm,\n",
    "                \"response_dict\": llm_response_dict\n",
    "            }\n",
    "\n",
    "        # 4.2 UCB and Reverse-Myerson Selection (CORRECTED)\n",
    "        \n",
    "        # 1. Calculate Virtual Valuation for each LLM's result\n",
    "        virtual_valuations = {}\n",
    "        for llm_key, result in round_results.items():\n",
    "            virtual_valuations[llm_key] = calculate_virtual_valuation(\n",
    "                llm_key, country, GLOBAL_T, result[\"reward\"], result[\"cost\"]\n",
    "            )\n",
    "\n",
    "        # 2. Select the winner: Highest Virtual Valuation wins (Maximizing Utility)\n",
    "        winning_llm_key = min(virtual_valuations, key=virtual_valuations.get) \n",
    "        \n",
    "        winning_result = round_results[winning_llm_key]\n",
    "        \n",
    "        # 4.3 Update Statistics\n",
    "        \n",
    "        # Update UCB/Myerson Arm Stats with the Net Utility (Reward - Cost)\n",
    "        update_ucb_stats(winning_llm_key, winning_result[\"utility\"]) # CRITICAL FIX: Use utility for update\n",
    "        llm_selection_counts[winning_llm_key] += 1\n",
    "        \n",
    "        # Update Overall Evaluation Metrics\n",
    "        total_reward_collected += winning_result[\"reward\"]\n",
    "        total_cost_incurred += winning_result[\"cost\"]\n",
    "        \n",
    "        for col in EVAL_COLUMNS:\n",
    "            if winning_result[\"is_correct\"][col]:\n",
    "                correct_counts[col] += 1\n",
    "        \n",
    "        if (index + 1) % 10 == 0 or (index + 1) == total_count:\n",
    "            print(f\"Processed {index + 1}/{total_count} entries. Winner: {winning_llm_key} | Total Reward: {total_reward_collected:.2f}\")\n",
    "\n",
    "    # 4.4 Final Efficiency Calculation\n",
    "    efficiency = {\n",
    "        col: f\"{correct_counts[col] / total_count * 100:.2f}%\" \n",
    "        for col in EVAL_COLUMNS\n",
    "    }\n",
    "    \n",
    "    total_possible_reward = total_count * sum(REWARD_MAP.values())\n",
    "    \n",
    "    return (efficiency, total_count, correct_counts, llm_selection_counts, \n",
    "            total_reward_collected, total_possible_reward, total_cost_incurred)\n",
    "            \n",
    "\n",
    "            \n",
    "                    \n",
    "                        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb90d79-46fb-4b96-8768-165d1a3a051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM pipelines...\n",
      "Loading 1B-Instruct (meta-llama/Llama-3.2-1B-Instruct)...\n",
      "1B-Instruct loaded successfully.\n",
      "Loading 3B-Instruct (meta-llama/Llama-3.2-3B-Instruct)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887634d84de2429ca50b468f7e538ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3B-Instruct loaded successfully.\n",
      "Starting UCB/Reverse-Myerson evaluation on 196 countries...\n",
      "Processed 10/196 entries. Winner: LLM_3B | Total Reward: 28.00\n",
      "Processed 20/196 entries. Winner: LLM_3B | Total Reward: 54.00\n",
      "Processed 30/196 entries. Winner: LLM_1B | Total Reward: 78.00\n",
      "Processed 40/196 entries. Winner: LLM_3B | Total Reward: 103.00\n",
      "Processed 50/196 entries. Winner: LLM_3B | Total Reward: 133.00\n",
      "Processed 60/196 entries. Winner: LLM_3B | Total Reward: 160.00\n",
      "Processed 70/196 entries. Winner: LLM_3B | Total Reward: 191.00\n",
      "Processed 80/196 entries. Winner: LLM_3B | Total Reward: 218.00\n",
      "Processed 90/196 entries. Winner: LLM_3B | Total Reward: 242.00\n",
      "Processed 100/196 entries. Winner: LLM_3B | Total Reward: 270.00\n",
      "Processed 110/196 entries. Winner: LLM_3B | Total Reward: 291.00\n",
      "Processed 120/196 entries. Winner: LLM_3B | Total Reward: 320.00\n",
      "Processed 130/196 entries. Winner: LLM_1B | Total Reward: 345.00\n",
      "Processed 140/196 entries. Winner: LLM_3B | Total Reward: 371.00\n",
      "Processed 150/196 entries. Winner: LLM_3B | Total Reward: 401.00\n",
      "Processed 160/196 entries. Winner: LLM_1B | Total Reward: 426.00\n",
      "Processed 170/196 entries. Winner: LLM_3B | Total Reward: 459.00\n",
      "Processed 180/196 entries. Winner: LLM_3B | Total Reward: 484.00\n",
      "Processed 190/196 entries. Winner: LLM_1B | Total Reward: 516.00\n",
      "Processed 196/196 entries. Winner: LLM_3B | Total Reward: 528.00\n",
      "\n",
      "======================================================================\n",
      "ðŸ§  Multi-LLM UCB/Reverse-Myerson Evaluation Results (FIXED)\n",
      "======================================================================\n",
      "Total Countries Evaluated (t): 196\n",
      "Total Possible Reward: 1176\n",
      "Total Reward Collected: 528.00\n",
      "Total Cost Incurred (Mock): $0.05481950\n",
      "Net Utility (Reward - Cost): 527.95\n",
      "\n",
      "## Model Selection Counts\n",
      "| LLM    | Times Selected   |\n",
      "|:-------|:-----------------|\n",
      "| LLM_1B | 98               |\n",
      "| LLM_3B | 98               |\n",
      "\n",
      "## Final Accuracy (Based on Winning LLM's Prediction)\n",
      "| Column       | Efficiency   |\n",
      "|:-------------|:-------------|\n",
      "| Capital City | 87.76%       |\n",
      "| Continent    | 92.86%       |\n",
      "| Latitude     | 25.51%       |\n",
      "| Longitude    | 18.88%       |\n",
      "\n",
      "Raw Correct Counts (for the selected winner):\n",
      "- Capital City: 172/196 correct\n",
      "- Continent: 182/196 correct\n",
      "- Latitude: 50/196 correct\n",
      "- Longitude: 37/196 correct\n",
      "\n",
      "LLM UCB Statistics (Mean Reward = Mean Utility):\n",
      "- LLM_1B: N=98, Mean Utility=2.3162\n",
      "- LLM_3B: N=98, Mean Utility=3.0710\n"
     ]
    }
   ],
   "source": [
    "# --- 5. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 5.1 Initialize LLMs\n",
    "    initialize_llms()\n",
    "    \n",
    "    # 5.2 Main Evaluation Block\n",
    "    try:\n",
    "        # Load the ground truth data\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = calculate_efficiency_with_ucb_myerson(data, QUERY_COLUMN)\n",
    "        (efficiency_results, total, correct, llm_selections, \n",
    "         total_reward, total_possible_reward, total_cost) = results\n",
    "        \n",
    "        # 5.3 Print Final Results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ðŸ§  Multi-LLM UCB/Reverse-Myerson Evaluation Results (FIXED)\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total Countries Evaluated (t): {total}\")\n",
    "        print(f\"Total Possible Reward: {total_possible_reward}\")\n",
    "        print(f\"Total Reward Collected: {total_reward:.2f}\")\n",
    "        print(f\"Total Cost Incurred (Mock): ${total_cost:.8f}\")\n",
    "        print(f\"Net Utility (Reward - Cost): {total_reward - total_cost:.2f}\")\n",
    "        \n",
    "        print(\"\\n## Model Selection Counts\")\n",
    "        selection_table = pd.DataFrame([llm_selections]).T\n",
    "        selection_table.columns = ['Times Selected']\n",
    "        selection_table.index.name = 'LLM'\n",
    "        print(selection_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "        print(\"\\n## Final Accuracy (Based on Winning LLM's Prediction)\")\n",
    "        results_table = pd.DataFrame([efficiency_results]).T\n",
    "        results_table.columns = ['Efficiency']\n",
    "        results_table.index.name = 'Column'\n",
    "        print(results_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "        print(\"\\nRaw Correct Counts (for the selected winner):\")\n",
    "        for col in EVAL_COLUMNS:\n",
    "            print(f\"- {col}: {correct[col]}/{total} correct\")\n",
    "        \n",
    "        print(\"\\nLLM UCB Statistics (Mean Reward = Mean Utility):\")\n",
    "        for key, config in LLM_CONFIGS.items():\n",
    "             print(f\"- {key}: N={config['ucb_N']}, Mean Utility={config['ucb_mean_reward']:.4f}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nFATAL ERROR: The file was not found at the configured path:\\n{file_path}\")\n",
    "        print(\"Please ensure the path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unhandled error occurred during execution: {e}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993526c-54be-497a-988a-3b48a69573e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dccddf0-1a74-4c12-b8ad-832b3ba3731a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My Llama Env)",
   "language": "python",
   "name": "my_llama_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
