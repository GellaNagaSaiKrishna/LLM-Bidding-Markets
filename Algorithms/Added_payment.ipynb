{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533dd9c7-1634-4523-adf0-231c2df49d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM pipelines...\n",
      "Loading 1B-Instruct (meta-llama/Llama-3.2-1B-Instruct)...\n",
      "1B-Instruct loaded successfully.\n",
      "Loading 3B-Instruct (meta-llama/Llama-3.2-3B-Instruct)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecfbc3985424cfa929f7faf61a7c2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3B-Instruct loaded successfully.\n",
      "Starting UCB/Reverse-Myerson evaluation on 196 countries...\n",
      "Processed 10/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.5861$ | Loser's Bid $a=-0.7587$ | Critical Bid $a=-0.7587$\n",
      "Processed 20/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.5669$ | Loser's Bid $a=-0.8654$ | Critical Bid $a=-0.8654$\n",
      "Processed 30/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.4125$ | Loser's Bid $a=-0.9221$ | Critical Bid $a=-0.9221$\n",
      "Processed 40/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.3588$ | Loser's Bid $a=-0.9603$ | Critical Bid $a=-0.9603$\n",
      "Processed 50/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.4473$ | Loser's Bid $a=-0.9889$ | Critical Bid $a=-0.9889$\n",
      "Processed 60/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.3859$ | Loser's Bid $a=-1.0117$ | Critical Bid $a=-1.0117$\n",
      "Processed 70/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.3704$ | Loser's Bid $a=-1.0306$ | Critical Bid $a=-1.0306$\n",
      "Processed 80/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.3455$ | Loser's Bid $a=-1.0467$ | Critical Bid $a=-1.0467$\n",
      "Processed 90/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.2696$ | Loser's Bid $a=-1.0606$ | Critical Bid $a=-1.0606$\n",
      "Processed 100/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.2593$ | Loser's Bid $a=-1.0730$ | Critical Bid $a=-1.0730$\n",
      "Processed 110/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.2230$ | Loser's Bid $a=-1.0840$ | Critical Bid $a=-1.0840$\n",
      "Processed 120/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.2178$ | Loser's Bid $a=-1.0940$ | Critical Bid $a=-1.0940$\n",
      "Processed 130/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.1823$ | Loser's Bid $a=-1.1031$ | Critical Bid $a=-1.1031$\n",
      "Processed 140/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.1589$ | Loser's Bid $a=-1.1115$ | Critical Bid $a=-1.1115$\n",
      "Processed 150/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.1654$ | Loser's Bid $a=-1.1192$ | Critical Bid $a=-1.1192$\n",
      "Processed 160/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.1333$ | Loser's Bid $a=-1.1264$ | Critical Bid $a=-1.1264$\n",
      "Processed 170/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.1462$ | Loser's Bid $a=-1.1331$ | Critical Bid $a=-1.1331$\n",
      "Processed 180/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.1242$ | Loser's Bid $a=-1.1394$ | Critical Bid $a=-1.1394$\n",
      "Processed 190/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.1361$ | Loser's Bid $a=-1.1453$ | Critical Bid $a=-1.1453$\n",
      "Processed 196/196 entries. Winner: LLM_1B | Winner's Bid $a=-2.1283$ | Loser's Bid $a=-1.1487$ | Critical Bid $a=-1.1487$\n",
      "\n",
      "======================================================================\n",
      "ðŸ§  Multi-LLM UCB/Reverse-Myerson Evaluation Results (with VCG Payment Logic)\n",
      "======================================================================\n",
      "Reserve Price 'a' used: 5.0\n",
      "Total Countries Evaluated (t): 196\n",
      "Total Possible Reward: 1176\n",
      "Total Reward Collected: 401.00\n",
      "Total Cost Incurred (Actual Model Cost): $0.01816000\n",
      "Total Payment Incurred (to Winner LLM): $0.01816000\n",
      "Net Utility (Reward - Payment): 400.98\n",
      "\n",
      "## Model Selection Counts\n",
      "| LLM    | Times Selected   |\n",
      "|:-------|:-----------------|\n",
      "| LLM_1B | 196              |\n",
      "| LLM_3B | 0                |\n",
      "\n",
      "## Final Accuracy (Based on Winning LLM's Prediction)\n",
      "| Column       | Efficiency   |\n",
      "|:-------------|:-------------|\n",
      "| Capital City | 83.67%       |\n",
      "| Continent    | 88.27%       |\n",
      "| Latitude     | 10.20%       |\n",
      "| Longitude    | 6.12%        |\n",
      "\n",
      "Raw Correct Counts (for the selected winner):\n",
      "- Capital City: 164/196 correct\n",
      "- Continent: 173/196 correct\n",
      "- Latitude: 20/196 correct\n",
      "- Longitude: 12/196 correct\n",
      "\n",
      "LLM UCB Statistics (Mean Reward = Mean Utility):\n",
      "- LLM_1B: N=196, Mean Utility=2.0458\n",
      "- LLM_3B: N=0, Mean Utility=0.0000\n"
     ]
    }
   ],
   "source": [
    "# --- 0. INSTALL AND SETUP (Run these lines first if you haven't already) ---\n",
    "# !pip install accelerate transformers pandas torch numpy scipy\n",
    "from huggingface_hub import login\n",
    "login(token='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')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Suppress transformers warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# --- GLOBAL CONFIGURATION ---\n",
    "LLM_CONFIGS = {\n",
    "    # Model ID, Name, and Mock Token Cost (for demonstration)\n",
    "    \"LLM_1B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        \"name\": \"1B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000005, # Mock cost: cheaper model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "        \"ucb_N\": 0,    # N: Number of times this arm has been selected\n",
    "        \"ucb_Q\": 0.0,  # Q: Total utility (Reward - Cost) received\n",
    "        \"ucb_mean_reward\": 0.0, # Q/N: Average utility (Note: Key kept as 'reward' for simplicity)\n",
    "    },\n",
    "    \"LLM_3B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        \"name\": \"3B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000025, # Mock cost: more expensive model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "        \"ucb_N\": 0,\n",
    "        \"ucb_Q\": 0.0,\n",
    "        \"ucb_mean_reward\": 0.0,\n",
    "    }\n",
    "}\n",
    "# UCB exploration parameter\n",
    "UCB_C = 0.5\n",
    "\n",
    "# Reserve price is set as a minimum acceptable *utility* (Reward - Cost) for the task.\n",
    "# Since 'a' = -(Historical Utility) - UCB_Bonus, a LOWER 'a' is a BETTER bid.\n",
    "#\n",
    "# CRITICAL CHANGE: Increased Reserve Price 'A' to +5.0 (was -2.0)\n",
    "# This means any adjusted bid 'a' less than +5.0 is accepted, preventing continuous rejections.\n",
    "RESERVE_PRICE_A = 5.0 \n",
    "\n",
    "# Mapping columns to rewards for a correct prediction\n",
    "REWARD_MAP = {\n",
    "    'Capital City': 1,\n",
    "    'Continent': 1,\n",
    "    'Latitude': 2,\n",
    "    'Longitude': 2\n",
    "}\n",
    "EVAL_COLUMNS = list(REWARD_MAP.keys())\n",
    "\n",
    "# Dataset configuration\n",
    "# IMPORTANT: Use the exact path to your CSV file\n",
    "file_path = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv'\n",
    "#file_path = 'all capital cities in the world.csv' # Placeholder for a common file structure\n",
    "QUERY_COLUMN = 'Country'\n",
    "\n",
    "# Global counter for the total number of rounds (t in UCB)\n",
    "GLOBAL_T = 0\n",
    "\n",
    "# --- 1. LLM INITIALIZATION (No changes) ---\n",
    "def initialize_llms():\n",
    "    \"\"\"Initializes both Llama 3 models.\"\"\"\n",
    "    global LLM_CONFIGS\n",
    "    print(\"Initializing LLM pipelines...\")\n",
    "    \n",
    "    for key, config in LLM_CONFIGS.items():\n",
    "        try:\n",
    "            print(f\"Loading {config['name']} ({config['model_id']})...\")\n",
    "            # Using low-precision dtype and device_map requires 'accelerate'\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=config['model_id'],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            config[\"pipe\"] = pipe\n",
    "            config[\"tokenizer\"] = pipe.tokenizer\n",
    "            print(f\"{config['name']} loaded successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFATAL: Failed to load {config['name']} pipeline. Check environment, token, and hardware.\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "# --- 2. LLM PREDICTION AND COST CALCULATION (No changes) ---\n",
    "def get_llm_prediction_and_cost(country_name, llm_key):\n",
    "    \"\"\"\n",
    "    Queries the specified Llama 3 pipeline, returns data, raw output, and mock cost.\n",
    "    \"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    pipe = config[\"pipe\"]\n",
    "    pipe_tokenizer = config[\"tokenizer\"]\n",
    "    cost_per_token = config[\"cost_per_token\"]\n",
    "\n",
    "    # 2.1 Construct Prompt\n",
    "    prompt_instruction = f\"\"\"\n",
    "    You are an expert geographical information system.\n",
    "    Your task is to provide the Capital City, Continent, Latitude, and Longitude for the requested country.\n",
    "    You MUST respond ONLY with a valid JSON object. DO NOT include any text outside the JSON object.\n",
    "    The JSON structure must be: {{\"Capital City\": \"...\", \"Continent\": \"...\", \"Latitude\": \"...\", \"Longitude\": \"...\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Provide the geographical data for: {country_name}\"},\n",
    "    ]\n",
    "\n",
    "    # Apply chat template for Llama 3 format\n",
    "    prompt = pipe_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 2.2 Run Inference\n",
    "    terminators = [\n",
    "        pipe_tokenizer.eos_token_id,\n",
    "        pipe_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    # Run Inference with deterministic settings\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # 2.3 Extract and Parse the JSON\n",
    "    raw_output = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "    \n",
    "    json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
    "    \n",
    "    llm_response_dict = {col: \"\" for col in EVAL_COLUMNS}\n",
    "    \n",
    "    if json_match:\n",
    "        json_string = json_match.group(0)\n",
    "        try:\n",
    "            llm_response_dict = json.loads(json_string)\n",
    "        except json.JSONDecodeError:\n",
    "            pass # Keep default empty dict if parsing fails\n",
    "\n",
    "    # 2.4 Mock Cost Calculation\n",
    "    # Token count estimation: 1 token is roughly 4 characters\n",
    "    prompt_tokens = len(prompt) // 4\n",
    "    response_tokens = len(raw_output) // 4\n",
    "    total_tokens = prompt_tokens + response_tokens\n",
    "    \n",
    "    cost = total_tokens * cost_per_token\n",
    "\n",
    "    return llm_response_dict, raw_output, total_tokens, cost\n",
    "\n",
    "\n",
    "# --- 3. UCB AND MYERSON LOGIC (SLIGHT CHANGE TO RETURN 'a') ---\n",
    "\n",
    "def calculate_virtual_valuation(llm_key, country_name, current_t, total_reward, total_cost):\n",
    "    \"\"\"\n",
    "    Calculates the Virtual Valuation (V) for a given LLM's result and returns V and 'a'.\n",
    "    \n",
    "    a = -Historical_Utility - C * sqrt(ln(t)/N) (Adjusted Utility/Bid)\n",
    "    \"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Step 1: Calculate 'a' (Adjusted Utility/Bid)\n",
    "    N_eff = max(config[\"ucb_N\"], 1)\n",
    "    \n",
    "    # 'a' is the mechanism's \"bid\". Lower 'a' means a better bid (higher expected utility).\n",
    "    historical_mean_utility = config[\"ucb_mean_reward\"]\n",
    "    \n",
    "    # a = - (Historical Mean Utility) - UCB_Bonus\n",
    "    a = -historical_mean_utility - UCB_C * math.sqrt(math.log(current_t) / N_eff)\n",
    "    \n",
    "    # Step 2: Calculate CDF(a) and PDF(a)\n",
    "    try:\n",
    "        pdf_a = norm.pdf(a)\n",
    "        cdf_a = norm.cdf(a)\n",
    "    except ValueError:\n",
    "        return float('inf'), float('inf')\n",
    "\n",
    "\n",
    "    # Step 3: Calculate Virtual Valuation\n",
    "    \n",
    "    if pdf_a == 0:\n",
    "        virtual_valuation = float('inf')\n",
    "    else:\n",
    "        # V(a) = a + (cdf_a / pdf_a) - We select the LLM with the MINIMUM V(a)\n",
    "        # to maximize the implicit utility.\n",
    "        virtual_valuation = a + (cdf_a / pdf_a)\n",
    "        \n",
    "    return virtual_valuation, a\n",
    "\n",
    "def update_ucb_stats(llm_key, utility):\n",
    "    \"\"\"Updates the UCB statistics for the winning LLM arm based on Net Utility (Reward - Cost).\"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Q now tracks total utility (Reward - Cost)\n",
    "    config[\"ucb_Q\"] += utility\n",
    "    config[\"ucb_N\"] += 1\n",
    "    \n",
    "    # mean_reward now tracks mean utility\n",
    "    config[\"ucb_mean_reward\"] = config[\"ucb_Q\"] / config[\"ucb_N\"]\n",
    "\n",
    "\n",
    "# --- 4. EVALUATION LOGIC (Main loop - MODIFIED FOR PAYMENT) ---\n",
    "def calculate_efficiency_with_ucb_myerson(df, query_col):\n",
    "    \"\"\"\n",
    "    Loops through the dataset, gets predictions from both LLMs,\n",
    "    applies UCB/Myerson logic to select the winner, and calculates overall efficiency.\n",
    "    \"\"\"\n",
    "    global GLOBAL_T\n",
    "    \n",
    "    # Data cleaning for ground truth\n",
    "    for col in EVAL_COLUMNS:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "    total_count = len(df)\n",
    "    \n",
    "    # Tracking for final results\n",
    "    correct_counts = {col: 0 for col in EVAL_COLUMNS}\n",
    "    llm_selection_counts = {key: 0 for key in LLM_CONFIGS.keys()}\n",
    "    total_reward_collected = 0\n",
    "    total_cost_incurred = 0\n",
    "    total_payment_incurred = 0 \n",
    "\n",
    "    print(f\"Starting UCB/Reverse-Myerson evaluation on {total_count} countries...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        country = row[query_col]\n",
    "        GLOBAL_T += 1 # Increment total rounds (t)\n",
    "\n",
    "        # Dictionary to hold the results for both LLMs in this round\n",
    "        round_results = {}\n",
    "        \n",
    "        # 4.1 Get Predictions, Rewards, and Costs for BOTH LLMs\n",
    "        for llm_key in LLM_CONFIGS.keys():\n",
    "            # Run inference for the current LLM\n",
    "            llm_response_dict, raw_output, total_tokens, cost = \\\n",
    "                get_llm_prediction_and_cost(country, llm_key)\n",
    "            \n",
    "            # Calculate Total Reward for this LLM's prediction\n",
    "            current_reward = 0\n",
    "            is_correct_for_llm = {col: False for col in EVAL_COLUMNS}\n",
    "            \n",
    "            for col in EVAL_COLUMNS:\n",
    "                true_value = row[col]\n",
    "                predicted_value = str(llm_response_dict.get(col, '')).strip().lower()\n",
    "                \n",
    "                is_correct = (predicted_value == true_value)\n",
    "                \n",
    "                # Robust Comparison for Latitude/Longitude (Tolerance 0.05)\n",
    "                if col in ['Latitude', 'Longitude']:\n",
    "                    true_num_str = re.sub(r'[^0-9.-]', '', true_value)\n",
    "                    pred_num_str = re.sub(r'[^0-9.-]', '', predicted_value)\n",
    "                    \n",
    "                    try:\n",
    "                        true_num = float(true_num_str)\n",
    "                        pred_num = float(pred_num_str)\n",
    "                        \n",
    "                        if abs(true_num - pred_num) < 0.05:\n",
    "                            is_correct = True\n",
    "                        else:\n",
    "                            is_correct = False\n",
    "                    except ValueError:\n",
    "                        is_correct = False\n",
    "                \n",
    "                if is_correct:\n",
    "                    current_reward += REWARD_MAP[col]\n",
    "                    is_correct_for_llm[col] = True\n",
    "            \n",
    "            round_results[llm_key] = {\n",
    "                \"reward\": current_reward,\n",
    "                \"cost\": cost,\n",
    "                \"utility\": current_reward - cost, # Store Net Utility\n",
    "                \"is_correct\": is_correct_for_llm,\n",
    "                \"response_dict\": llm_response_dict,\n",
    "                \"adjusted_a\": None # Will store 'a' (the bid)\n",
    "            }\n",
    "\n",
    "\n",
    "        # 4.2 UCB and Reverse-Myerson Selection & Payment Calculation\n",
    "        \n",
    "        # 1. Calculate Virtual Valuation (V) and Adjusted Utility/Bid ('a') for each LLM\n",
    "        virtual_valuations = {}\n",
    "        adjusted_a_values = {}\n",
    "        for llm_key, result in round_results.items():\n",
    "            V, a = calculate_virtual_valuation(\n",
    "                llm_key, country, GLOBAL_T, result[\"reward\"], result[\"cost\"]\n",
    "            )\n",
    "            virtual_valuations[llm_key] = V\n",
    "            adjusted_a_values[llm_key] = a # The \"bid\" for the payment rule\n",
    "            round_results[llm_key][\"adjusted_a\"] = a\n",
    "        \n",
    "        # 2. Select the winner: MINIMUM Virtual Valuation wins (Maximizing Utility)\n",
    "        winning_llm_key = min(virtual_valuations, key=virtual_valuations.get)\n",
    "        winning_result = round_results[winning_llm_key]\n",
    "\n",
    "        # 3. Check for Reserve Price: If winner's bid 'a' is too high, no one wins\n",
    "        # Since lower 'a' is better, we reject if 'a' EXCEEDS RESERVE_PRICE_A.\n",
    "        if winning_result[\"adjusted_a\"] > RESERVE_PRICE_A:\n",
    "             # Winner's bid (a) is too low utility-wise (a is too high), so reject.\n",
    "             print(f\"Round {GLOBAL_T}: All bids rejected (Winner's adjusted bid $a={winning_result['adjusted_a']:.4f}$ exceeds Reserve $a={RESERVE_PRICE_A:.4f}$).\")\n",
    "             continue # Skip updating stats and metrics for this round\n",
    "\n",
    "        \n",
    "        # 4. Calculate Payment (VCG-like mechanism)\n",
    "        \n",
    "        # Identify winner and loser bids (in terms of 'a')\n",
    "        loser_llm_key = next(key for key in LLM_CONFIGS.keys() if key != winning_llm_key)\n",
    "        loser_a = adjusted_a_values[loser_llm_key]\n",
    "\n",
    "        # Critical Bid 'A' (the highest 'a' that would have still let the winner win)\n",
    "        # Since lower 'a' is better, the critical bid is the minimum of the reserve and the loser's bid.\n",
    "        critical_bid_a = min(RESERVE_PRICE_A, loser_a)\n",
    "\n",
    "        # The actual financial payment is based on the winner's actual cost, \n",
    "        # but the critical bid 'a' determined the selection and the *implicit* charge.\n",
    "        final_payment = winning_result[\"cost\"] # Using actual cost for monetary tracking\n",
    "        \n",
    "        \n",
    "        # 4.3 Update Statistics\n",
    "        \n",
    "        # Update UCB/Myerson Arm Stats with the Net Utility (Reward - Cost)\n",
    "        update_ucb_stats(winning_llm_key, winning_result[\"utility\"])\n",
    "        llm_selection_counts[winning_llm_key] += 1\n",
    "        \n",
    "        # Update Overall Evaluation Metrics\n",
    "        total_reward_collected += winning_result[\"reward\"]\n",
    "        total_cost_incurred += winning_result[\"cost\"] # Total cost of models run (for reference)\n",
    "        total_payment_incurred += final_payment      # Total actual payment to the winner\n",
    "        \n",
    "        for col in EVAL_COLUMNS:\n",
    "            if winning_result[\"is_correct\"][col]:\n",
    "                correct_counts[col] += 1\n",
    "        \n",
    "        if (index + 1) % 10 == 0 or (index + 1) == total_count:\n",
    "            print(f\"Processed {index + 1}/{total_count} entries. Winner: {winning_llm_key} | Winner's Bid $a={winning_result['adjusted_a']:.4f}$ | Loser's Bid $a={loser_a:.4f}$ | Critical Bid $a={critical_bid_a:.4f}$\")\n",
    "\n",
    "    # 4.4 Final Efficiency Calculation\n",
    "    efficiency = {\n",
    "        col: f\"{correct_counts[col] / total_count * 100:.2f}%\"\n",
    "        for col in EVAL_COLUMNS\n",
    "    }\n",
    "    \n",
    "    total_possible_reward = total_count * sum(REWARD_MAP.values())\n",
    "    \n",
    "    return (efficiency, total_count, correct_counts, llm_selection_counts,\n",
    "            total_reward_collected, total_possible_reward, total_cost_incurred, total_payment_incurred)\n",
    "\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 5.1 Initialize LLMs\n",
    "    try:\n",
    "        initialize_llms()\n",
    "    except Exception as e:\n",
    "        print(f\"LLM initialization failed: {e}. Proceeding with potential errors or mocking.\")\n",
    "\n",
    "\n",
    "    # 5.2 Main Evaluation Block\n",
    "    try:\n",
    "        # Load the ground truth data\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = calculate_efficiency_with_ucb_myerson(data, QUERY_COLUMN)\n",
    "        (efficiency_results, total, correct, llm_selections,\n",
    "         total_reward, total_possible_reward, total_cost, total_payment) = results\n",
    "        \n",
    "        # 5.3 Print Final Results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ðŸ§  Multi-LLM UCB/Reverse-Myerson Evaluation Results (with VCG Payment Logic)\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Reserve Price 'a' used: {RESERVE_PRICE_A}\")\n",
    "        print(f\"Total Countries Evaluated (t): {total}\")\n",
    "        print(f\"Total Possible Reward: {total_possible_reward}\")\n",
    "        print(f\"Total Reward Collected: {total_reward:.2f}\")\n",
    "        print(f\"Total Cost Incurred (Actual Model Cost): ${total_cost:.8f}\")\n",
    "        print(f\"Total Payment Incurred (to Winner LLM): ${total_payment:.8f}\")\n",
    "        print(f\"Net Utility (Reward - Payment): {total_reward - total_payment:.2f}\")\n",
    "        \n",
    "        print(\"\\n## Model Selection Counts\")\n",
    "        selection_table = pd.DataFrame([llm_selections]).T\n",
    "        selection_table.columns = ['Times Selected']\n",
    "        selection_table.index.name = 'LLM'\n",
    "        print(selection_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "        \n",
    "        print(\"\\n## Final Accuracy (Based on Winning LLM's Prediction)\")\n",
    "        results_table = pd.DataFrame([efficiency_results]).T\n",
    "        results_table.columns = ['Efficiency']\n",
    "        results_table.index.name = 'Column'\n",
    "        print(results_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "        print(\"\\nRaw Correct Counts (for the selected winner):\")\n",
    "        for col in EVAL_COLUMNS:\n",
    "            print(f\"- {col}: {correct[col]}/{total} correct\")\n",
    "        \n",
    "        print(\"\\nLLM UCB Statistics (Mean Reward = Mean Utility):\")\n",
    "        for key, config in LLM_CONFIGS.items():\n",
    "             print(f\"- {key}: N={config['ucb_N']}, Mean Utility={config['ucb_mean_reward']:.4f}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nFATAL ERROR: The file was not found at the configured path:\\n{file_path}\")\n",
    "        print(\"Please ensure the path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unhandled error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd5d1e-295b-4e56-96ad-e322167ebd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My Llama Env)",
   "language": "python",
   "name": "my_llama_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
